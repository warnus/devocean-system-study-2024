# 안정 해시 설계 (Consistent Hashing)
- 안정 해시는 분산 시스템에서 데이터를 **균등**하게 분산시키기 위한 해싱 기술 중 하나
- 시스템 노드(서버) 추가 또는 제거 시 데이터의 재배치를 최소화하는 것이 목적

## 해시 키 재배치(Rehash) 문제

- N개의 캐시 서버가 있다고 가정, 이 서버들에 부하를 균등하게 나누는 보편적 방법: `serverIndex = hash(key) % N`

서버가 N개로 고정되어 있고 키의 분포가 균등할 때만 요청, 데이터를 균등하게 분산시킬 수 있다.

만약, 서버가 추가 또는 기존 서버가 삭제되어 변경된다면

N=4에서 N=3으로 줄면 키에 대한 해시값은 변하지 않아도 모듈러 연산을 적용한 serverIndex의 값이 달라져 캐시 클라이언트가 데이터가 없는 엉뚱한 서버에 접속하게 되어 버린다.

-> cache miss를 야기시킴

## 안정 해시
> 안정해시는 해시 테이블 크기가 조정될 때 평균적으로 오직 $k/n$ 개의 키만 재배치하는 해시 기술
> 
> (k는 키의 개수, n은 슬롯의 개수)

## 안정 해시의 동작 원리

![스크린샷 2024-06-02 오후 6 12 30](https://github.com/warnus/devocean-system-study-2024/assets/58351498/a755a2e7-bda9-49c2-a2b8-be013c5bcfe5)

- 해시 함수 $f$: SHA-1
- 함수의 출력 값 번위: $x0(=0)$, $x1$, $x2$, ..., $xn(=2^{160}-1)$
- SHA-1의 해시공간(hash space) 범위는 [0, $2^{160} - 1$]

### 해시 서버
해시 함수 $f$ 를 사용하면 서버 IP나 이름을 이 링 위의 어떤 위치에 대응시킬 수 있다.
![스크린샷 2024-06-02 오후 6 12 52](https://github.com/warnus/devocean-system-study-2024/assets/58351498/3c1262b2-b2b3-44e8-abca-374ab8287ab5)

### 서버 조회

어떤 키가 저장되는 서버는 해당 키의 위치로부터 시계 방향으로 링을 탐색해 나가며 만나는 첫 번째 서버
- $key0$ 은 서버 0에 저장, $key 1$은 서버 1에 저장되는 방식

![스크린샷 2024-06-02 오후 6 13 55](https://github.com/warnus/devocean-system-study-2024/assets/58351498/6dc95a59-4faa-473e-8247-21fba04fa91f)

### 서버 추가

서버가 새로 추가될 때, 재배치를 해줘야 하는 키가 생긴다. 서버 $s4$ 가 새로 추가되어 $key0$ 만 재배치하면 되는 걸 알 수 있다.

![스크린샷 2024-06-02 오후 6 15 55](https://github.com/warnus/devocean-system-study-2024/assets/58351498/a2c9909f-c225-4291-97af-8dfc836ad7dd)

### 서버 제거
하나의 서버가 제거될 때에도, 일부만 재배치 된다. 즉, 삭제되는 서버가 저장하고 있던 키들이 재배치 된다.
![스크린샷 2024-06-02 오후 6 18 25](https://github.com/warnus/devocean-system-study-2024/assets/58351498/33668a6b-656a-4508-885b-8278173f7cca)

## 기본 구현법 - 두가지 문제
안정 해시 알고리즘의 기본적 절차
1. 서버와 키를 균등 분포 해시 함수를 사용해 해시 링에 배치한다.
2. 키의 위치에서 링을 시계 방향으로 탐색하다 만나는 최초의 서버가 키가 저장될 서버이다.

BUT, 이 접근법에는 두가지 문제가 있다.

**첫번째, 서버가 추가되거나 삭제되는 상황에서 partition의 크기를 균등하게 유지하는 게 불가능하다는 것**
- 파티션: 인접한 서버 사이의 해시 공간

기본적인 구현법을 사용할 경우, 서버마다 할당 받는 파티션의 굉장히 작거나, 굉장히 큰 해시 공간을 할당 받는 상황이 발생할 수 있다. (극단적...)

![스크린샷 2024-06-02 오후 6 23 10](https://github.com/warnus/devocean-system-study-2024/assets/58351498/6f9a3cef-99b1-4a87-b03b-7afee685a8b0)

**두번째, 키의 균등 분포를 달성하기 어렵다는 것**

서버1과 서버3은 아무 데이터를 갖지 않고 있지만 대부분의 키는 서버 2에 보관될 것이다.

![스크린샷 2024-06-02 오후 6 24 05](https://github.com/warnus/devocean-system-study-2024/assets/58351498/9fbacb67-63fd-4678-87c3-74f2aac5edd1)

서버2처럼 한 서버에 몰리는 문제를 해결하기 위해 제안된 기법이 **가상 노드(virtual node)** 또는 복제라 불리는 기법이다.

### 가상 노드 (virtual node)
> 가상노드는 실제 노드 또는 서버를 **가리키는 노드**이다.
>
> 하나의 서버는 링 위에 여러 개의 가상 노드를 가질 수 있으며, 가상 노드의 개수가 늘어날수록 표준 편차가 작아져 데이터가 고르게 분포된다.

가상 노드 개수는 100 ~ 200개를 사용할 때 기준으로 표준 편차 값이 평균의 5% ~ 10% 사이라고 한다..

가상 노드 개수를 더 늘리게 되면 표준 편자 값은 더 떨어지지만 가상 노드 데이터를 저장할 공간은 더 많이 필요하게 된다. 따라서 시스템 요구사항에 따라 tradeoff를 하는 것이 중요..

![스크린샷 2024-06-02 오후 6 27 04](https://github.com/warnus/devocean-system-study-2024/assets/58351498/8d407b26-30de-467e-9ed7-a498cd690eae)

- 서버 0 대신, s0_0, s0_1, s0_2의 세 개 가상 노드를 사용했으며 각 서버는 하나가 아닌 여러 개 파티션을 관리하게 되었다.
- s0은 서버 0이 관리하는 파티션, s1은 서버 1이 관리하는 파티션

키의 위치로부터 시계방향으로 링을 탐색하다 만나는 최초의 가상 노드가 해당 키가 저장될 서버가 된다. 

### 재배치할 키 결정
서버의 개수가 변경될 경우, 키를 재배치해야 하는데 어느 범위의 키들이 재배치되어야 할까?

![스크린샷 2024-06-02 오후 6 31 09](https://github.com/warnus/devocean-system-study-2024/assets/58351498/34e59bb0-2fd9-4d1d-9672-644e60b5e62b)

서버 4가 새롭게 추가되었다고 가정해보자.

영향 받은 범위는 s4로 부터 그 반시계 방향에 있는 첫번째 서버 s3까지이다. 즉, s3 ~ s4 사이의 키들을 s4로 재배치해야 한다.

![스크린샷 2024-06-02 오후 6 32 31](https://github.com/warnus/devocean-system-study-2024/assets/58351498/7d3ad033-36ab-4e1e-b50d-f6b479d08b32)

s1이 위처럼 삭제된다면 s1부터 그 반시계 방향에 있는 최초 서버 s0 사이에 있는 키들이 s2로 재배치되어야 한다.
