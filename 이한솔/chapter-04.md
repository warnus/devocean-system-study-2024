# 처리율 제한 장치의 설계
> rate limiter: 클라이언트 또는 서비스가 보내는 트래픽의 처리율을 제어하기 위한 장치 </br>
> ex) 특정 기간 내에 전송되는 클라이언트의 HTTP request 횟수를 제한 </br>
>     - 사용자는 초당 2회 이상 새 글을 올릴 수 없음 </br>
>     - 같은 IP 주소로는 하루에 10개 이상의 계정 생성 불가 </br>
>     - 같은 디바이스로는 주당 5회 이상 리워드 요청 불가 </br>

**rate limiter의 장점**
1. DoS 공격 방지 - 추가 요청에 대해서는 처리를 중단함으로써 공격 차단
2. 비용 절감 - 서버를 불필요하게 많이 두지 않아도 됨
3. 서버 과부하 방지 - 봇, 잘못된 이용으로 발생한 트래픽을 걸러낼 수 있음

## 1단계) 문제 이해 및 설계 범위 확정
### 요구사항
1. 처리율을 초과하는 요청은 정확히 제한
2. 낮은 응답시간
3. 적은 메모리 사용
4. 분산형 처리율 제한: 하나의 rate limiter를 여러 서버나 프로세스에서 공유 -> "동기화 문제"
5. 예외 처리: 걸러진 요청이라면 사용자에게 알려야 함
6. 높은 결함 감내성: 제한 장치에 장애가 생기더라도 전체 시스템에 영향을 주어선 안됨

## 2단계) 개략적 설계안 제시 및 동의 구하기
### 처리율 제한 장치는 어디에 둘 것인가?

#### 클라이언트 측에 제한 장치를 두는 방법
일반적으로 클라이언트는 처리율 제한을 안정적으로 걸 수 있는 장소가 아님
- 클라이언트 요청은 쉽게 위변조가 가능하기 때문
- 또한, 모든 클라이언트의 구현을 통제하는 것도 어려움

#### 서버 측에 제한 장치를 두는 방법
1. 서버에 제한 장치를 두는 방법

![스크린샷 2024-05-19 오후 2 00 22](https://github.com/warnus/devocean-system-study-2024/assets/58351498/83156425-1de4-489e-8a85-e94e217f0865)

2. 처리율 제한 장치가 미들웨어가 되어 API 서버로 가는 요청을 통제하는 방법
![스크린샷 2024-05-19 오후 2 01 46](https://github.com/warnus/devocean-system-study-2024/assets/58351498/14ccf97a-2ffb-448b-843a-c8c378a29811)

API 서버의 처리율이 초당 2개로 제한된 상황, 클라이언트가 3번째 요청을 앞의 두 요청과 같은 초 범위 내에서 전송하였다고 가정

앞의 2개의 요청을 API 서버로 전송될 것이지만, 세번째 요청은 처리율 제한 미들웨어에 의해 가로막혀 HTTP status code 429가 반환
- 429 상태 코드: 사용자가 너무 많은 요청을 보내려고 했음(Too many requests)

클라우드 마이크로서비스에서 라면, 처리율 제한 장치는 API gateway로 구현됨
- API gateway: 처리율 제한, SSL 종단, 사용자 인증, IP 화이트리스트 관리 등을 지원하는 클라우드 서비스

#### 처리율 제한 장치를 두는 위치
- 프로그래밍 언어, 캐시 서비스 등 현재 사용하고 있는 기술 스택 점검
- 사업에 필요한 처리율 제한 알고리즘을 찾아라 (서버 측에서 모든 것을 구현한다면 알고리즘 자유롭게 선택가능, 게이트웨이 서비스를 사용한다면 선택지 제한)
- MSA 기반에 사용자 인증, IP 화이트리스트 처리를 위해 API gateway를 이미 설계에 포함했다면 처리율 제한 기능도 게이트웨이에 포함시켜야할 수 있음
- 처리율 제한 서비스를 직접 만드는데 시간이 듦 -> 상용 API gateway를 쓰는 것 추천

### 처리율 제한 알고리즘 
#### 토큰 버킷 알고리즘
토큰 버킷은 지정된 용량을 갖는 컨테이너, 이 버킷에 사전 설정된 양의 토킨이 주기적으로 채워짐

토큰이 꽉 찬 버킷에는 더 이상 버킷이 추가되지 않고 버려짐

토큰 버킷 용량이 4이고 매초 2개의 토큰이 추가되는 예제

![스크린샷 2024-05-19 오후 2 25 13](https://github.com/warnus/devocean-system-study-2024/assets/58351498/72cb08c6-e314-4b36-a56e-19085b499595)

- 각 요청은 처리될 떄마다 하나의 토큰을 사용
- 요청이 도착하면 버킷에 충분한 토큰이 있는지 검사
    - 토큰이 있다면, 버킷에서 토큰 하나를 꺼내고 요청을 시스템에 전달
    - 토큰이 없다면, 요청은 버려짐
 
![스크린샷 2024-05-19 오후 2 32 32](https://github.com/warnus/devocean-system-study-2024/assets/58351498/3e203314-22bb-4670-a034-937002d00c9d)

토큰 버킷의 크기가 4이고, 토큰 공급률이 분당 4인 토큰 버킷 알고리즘 처리 제한 로직
![스크린샷 2024-05-19 오후 2 39 34](https://github.com/warnus/devocean-system-study-2024/assets/58351498/48435722-f908-4b97-8496-4219689cf376)

**토큰 버킷 알고리즘에서 필요한 2개 인자**
- 버킷 크기: 버킷에 담을 수 있는 토큰의 최대 개수
- 토큰 공급률: 초당 몇개의 토큰이 버킷에 공급되는지

**버킷 개수 적용 사례**
- 통상적으로 API endpoint마다 별도의 버킷을 둠
  - 사용자마다 하루 한번만 포스팅 / 친구는 150명까지 추가 / 좋아요는 5번까지 가능하다면 사용자마다 3개의 버킷을 둠
  - IP주소별로 처리율 제한을 적용한다면 IP주소마다 버킷을 하나씩 할당
  - 시스템 처리율을 초당 10,000개로 제한한다면 모든 요청이 하나의 버킷을 공유하도록 구현

**장점**
1. 구현이 쉽다
2. 메모리 사용 측면에서도 효율적
3. 짧은 시간에 집중되는 트래픽 처리 가능(버킷에 남은 토큰이 존재하기만 하면 요청은 시스템으로 전달됨)

**단점**
1. 버킷 크기와 토큰 공급률이라는 두개의 인자를 적절히 튜닝하는 것은 까다로움

#### 누출 버킷 알고리즘
토큰 버킷 알고맂므과 비슷하지만 요청 처리율이 고정되어 있다는 차이점이 있음, 그리고 FIFO 큐로 구현됨

![스크린샷 2024-05-19 오후 4 07 00](https://github.com/warnus/devocean-system-study-2024/assets/58351498/32345505-8a42-4553-a89a-b54335566821)

- 요청이 도착하면 큐가 가득차있는지 확인
- 빈자리가 있다면, 큐에 요청 추가
- 가득차 있다면, 새 요청은 버림
- 지정된 시간마다 큐에서 요청을 꺼내어 처리

**누출 버킷 알고리즘의 두가지 인자**
- 버킷 크기: 큐 사이즈
- 처리율: 지정된 시간당 몇 개의 항목을 처리할지 지정하는 값(보통 초단위)

**장점**
1. 큐의 크기가 제한되어 있어 메모리 사용량에서 효율적
2. 고정된 처리율을 갖고 있어 안정적 출력이 필요한 경우에 적합

**단점**
1. 단시간에 많은 트래픽이 몰리게 되면 큐에 오래된 요청이 쌓이게 되어 그 요청을 제떄 처리 못하게 되면 최신 요청들은 버려지게 됨
2. 두 개의 인자를 튜닝하기가 까다로움

#### 고정 윈도 카우넡 알고리즘
- 타임라인을 고정된 간격의 window로 나누고 각 윈도우마다 counter를 붙임
- 요청이 접수될 때마다 카운터의 값 1씩 증가
- 카운터의 값이 사전에 설정된 임계치에 도달하면 새로운 요청은 새 윈도가 열릴 때까지 버려짐

![image](https://github.com/warnus/devocean-system-study-2024/assets/58351498/407458c7-1344-4e18-a02e-645c35fb7a61)

**문제점**

윈도우의 경계 부근에 순간적으로 많은 트래픽이 집중될 경우 윈도우에 할당된 양보다 더 많은 요청이 처리될 수 있음

![image](https://github.com/warnus/devocean-system-study-2024/assets/58351498/0a2ab725-c0f9-4bf6-b15f-fd28b7caa552)

최대 5개의 요청만 허용하는 시스템에서 카운터는 매분마다 초기화

2:00:00 ~ 2:01:00 사이에 5개의 요청이 들어왔고, 2:01:00 ~ 2:02:00 사이에 또 5개의 요청이 들어옴

윈도우 위치를 옮겨 2:00:30 ~ 2:01:30까지 1분 동안 시스템이 총 10개의 요청을 처리함 (허용 한도의 2배)

**장점**
1. 메모리 효율이 좋음
2. 윈도우가 닫히는 시점에 카운터를 초기화하는 방식은 특정한 트래픽 패턴을 처리하기에 적합

**단점**
1. 윈도우 경계 부근에서 일시적으로 많은 트래픽이 몰려드는 경우, 기대했던 시스템의 처리 한도보다 많은 양의 요청을 처리하게 됨

#### 이동 윈도 로깅 알고리즘
고정 윈도 카운터 알고리즘은 윈도우 경계 부근에 트래픽이 집중되는 경우 시스템에 설정된 한도보다 많은 요청을 처리하게 되는 문제점이 있음

이동 윈도 로깅 알고리즘은 이 문제를 해결할 수 있음!

- 타임스템프 추적 - 타임스탬프 데이터는 레디스의 sorted set 같은 캐시에 보관
- 새 요청이 오면 만료된 타임스탬프 제거 (만료 기준: 그 값이 현재 윈도의 시작 시점보다 오래된 타임스탬프)
- 새 요청의 타임스탬프를 로그에 추가
- 로그의 크기가 허용치보다 같거나 작으면 요청을 시스템에 전달, 그렇지 않다면 처리 거부

**분당 2개 요청이 한도인 시스템 예제**
![스크린샷 2024-05-19 오후 4 28 07](https://github.com/warnus/devocean-system-study-2024/assets/58351498/519cbb25-f487-4771-8379-f498ccbbf458)

- 요청이 1:00:01에 도착했을 때, 로그는 비어있는 상태이므로 요청은 허용됨
- 새로운 요청이 1:00:30에 도착하여 로그에 추가되고 허용 한도보다 작은 값이므로 시스템에 전달됨
- 그 다음, 새로운 요청이 1:00:50에 도착하여 로그에 추가되고 허용 한도보다 큰 값(3)이므로 로그에는 남아있지만 요청은 거부됨
- 새로운 요청이 1:01:40에 도착하여 1:00:40 이전의 타임스탬프는 로그에서 삭제되고 요청은 시스템으로 전달됨

**장점**
- 어느 순간의 윈도우를 보더라도 허용되는 요청의 개수는 시스템의 처리율 한도를 넘지 않음

**단점**
- 거부된 요청의 타임스탬프 값도 로그에 보관하기 때문에 다량의 메모리를 사용함

#### 이동 윈도 카운터 알고리즘 = 고정 윈도 카운터 알고리즘 + 이동 윈도 로깅 알고리즘
![image](https://github.com/warnus/devocean-system-study-2024/assets/58351498/fcbaf707-b879-40c4-9703-15d414006c81)

- 처리율 제한 장치의 한도가 분당 7개 요청
- 이전 1분동안 5개의 요청, 현재 1분동안 3개의 요청
  
현재 1분의 30% 시점에 도착한 새 요청의 경우, 현재 윈도에 몇개의 요청이 온것으로 보고 처리해야 할까?
- 현재 1분간의 요청 수 + 직전 1분간의 요청 수 * 이동 윈도와 직전 1분이 겹치는 비율
    - 3 + 5 * 70% = 6.5 개, 내림하면 6개

분당 7개의 요청을 받아들일 수 있으므로 현재 1분의 30% 시점에 도착한 신규 요청은 시스템으로 전달 될 것이지만, 그 직후에는 한도에 도달하여 더 이상의 요청은 받을 수 없게됨

**장점**
- 이전 시간대의 평균 처리율에 따라 현재 윈도우의 상태를 계산하므로 짧은 시간에 몰리는 트래픽에도 잘 대응함
- 메모리 효율이 좋음

**단점**
- 직전 시간대에 도착한 요청이 균등하게 분포되어 있다고 가정한 상태에서 추정치를 계산하므로 다소 느슨
  - 하지만, Cloudflare사의 실험에 따르면 40억개 가운데 시스템의 실제 상태와 맞지 않게 허용되거나 버려진 요청은 0.003%에 불과함

#### 개략적인 아키텍처
얼마나 많은 요청이 접수되었는지를 추적할 수 있는 카운터를 추적 대상별로 두고 이 카운터의 값이 어떤 한도를 넘어서면 한도를 넘어 도착한 요청은 거부하는 것

그렇다면, 카운터는 어디에 보관해야 할까?

데이터베이스 - 디스크 접근 때문에 느림

캐시 - 메모리상에서 동작하고 빠른데다 만료정책을 지원함

Redis가 지원하는 INCR, EXPIRE 명령어를 갖고 처리율 제한 장치를 구현할 수 있음
- INCR: 메모리에 저장된 카운터의 값을 1만큼 증가시킴
- EXPIRE: 카운터에 타임아웃 값을 설정, 설정된 시간이 지나면 카운터는 자동 삭제

![스크린샷 2024-05-19 오후 4 55 16](https://github.com/warnus/devocean-system-study-2024/assets/58351498/2dd152df-a9e4-41e8-b1d1-14ad53e0a61a)

- 클라이언트가 처리율 제한 미들웨어에게 요청을 보냄
- 처리율 제한 미들웨어는 레디스의 지정 버킷에서 카운터를 가져와 한도에 도달했는지 아닌지 검사
  - 한도에 도달했다면 요청은 거부됨
- 한도에 도달하지 않았다면 요청은 API 서버로 전달, 미들웨어는 카운터의 값을 증가시킨 후 다시 레디스에 저장

## 3단계) 상세 설계
- 처리율 제한 규칙은 어떻게 만들어지고 어디에 저장되는지
- 처리가 제한된 요청들은 어떻게 처리되는지

### 처리율 제한 규칙
시스템이 처리할 수 있는 마케팅 메시지의 최대치를 하루 5개로 제한
``` yaml
domain: messaging
descriptors:
  - key: message_type
    Value: marketing
    rate_limit:
        unit: day
        requests_per_unit: 5
```
클라이언트가 분당 5회 이상 로그인 할 수 없도록 제한
``` yaml
domain: auth
descriptors:
  - key: auth_type
    Value: login
    rate_limit:
        unit: minute
        reqeusts_per_unit: 5
```
### 처리율 한도 초과 트래픽의 처리
어떤 요청이 한도 제한에 걸리면 API는 HTTP 429 응답을 클라이언트에게 보냄

경우에 따라 한도 제한에 걸린 메시지를 나중에 처리하기 위해 큐에 보관할 수도 있음

#### 처리율 제한 장치가 사용하는 HTTP 응답 헤더
- 클라이언트의 요청이 처리율 제한에 걸리는지
- 자기 요청이 처리율 제한에 걸리기까지 얼마나 많은 요청을 보낼 수 있는지
위의 2가지 의문은 HTTP response header에 있다.

- X-Ratelimit-Remaining: 윈도우 내에 남은 처리 가능 요청의 수
- X-Ratelimit-Limit: 매 윈도우마다 클라이언트가 전송할 수 있는 요청의 수
- X-Ratelimit-Retry-After: 한도 제한에 걸리지 않으려면 몇 초 뒤에 요청을 다시 보내야 하는지 알림

사용자가 너무 많은 요청을 보내면 429 오류를 X-Ratelimit-Retry-After 헤더와 함께 반환

#### 상세설계
- 처리율 제한 규칙은 디스크에 보관, 작업 프로세스(workers)는 수시로 규칙을 디스크에 읽어 캐시에 저장
  
![스크린샷 2024-05-19 오후 5 41 45](https://github.com/warnus/devocean-system-study-2024/assets/58351498/035670b2-d91a-48d3-87a8-10788eaf2603)

- 클라이언트가 요청을 서버에 보내면 요청은 먼저 처리율 제한 미들웨어에 도달
- 처리율 제한 미들웨어는 제한 규칙을 캐시에서 가져옴, 카운터 및 마지막 요청의 타임스탬프를 레디스에서 가져옴
  - 해당 요청이 처리율 제한에 걸리지 않은 경우, API 서버로 보냄
  - 제한에 걸렸다면, 429 에러를 클라이언트에 보냄 (그대로 버리거나 메시지 큐에 보관)

#### 분산 환경에서의 처리율 제한 장치의 구현
여러 대의 서버와 병렬 스레드를 지원하도록 시스템을 확장하는 것은 **경쟁 조건**, **동기화** 문제를 해결해야 함

**경쟁 조건**
- 처리율 제한 장치 동작 방식
    - 레디스에서 카운터 값 조회
    - counter + 1의 값이 임계치를 넘는지 확인
    - 넘지 않는다면 레디스에 보관된 카운터 값을 1만큼 증가시킴

병행성이 심한 환경에서 아래와 같은 경쟁 조건 이슈가 발생할 수 있음

![스크린샷 2024-05-19 오후 5 47 53](https://github.com/warnus/devocean-system-study-2024/assets/58351498/c5bc74d5-16eb-4212-bb32-0f26ce4bbc5c)

- 레디스에 저장된 카운터 값이 3
- 두 개 요청을 처리하는 스레드가 각각 병렬로 counter를 읽고 그 둘 가운데 어느쪽도 아직 변경된 값을 저장하지 않은 상태
- 두 스레드는 다른 요청의 처리 상태와 무관하게 counter + 1을 레디스에 기록

하지만, 최종적으로 counter의 값은 5가 되어야 함

**경쟁 조건을 해결하는 방법**  
- lock: 시스템 성능을 저하시킴
- 루아 스크립트: Redis 2.6버전부터 Lua script 실행 지원, Lua script에서 작성한 로직을 실행하면 해당 연산이 Redis 서버에 `원자적으로` 처리되므로 race condition 해결 가능
  - 만약, 동시에 4개의 컨슈머가 루아 스크립트를 실행시킨다면 레디스 서버에 순차적으로 atomic 연산을 실행시키므로 가장 먼저 도착한 컨슈마의 루아스크립트가 API 호출 수를 100 증가시키므로 다음 루아 스크립트에서는 API 호출 수가 100임을 확인하고 더이상 API 호출 수를 증가시키지 않고 그대로 현재 값을 반환하게 됨
  - Redis에서 루아 스크립트를 실행시킬 경우 해당 스크립트는 반드시 짧게 끝나는 로직으로 작성(처리시간이 오래걸리는 로직이 들어가면 싱글스레드 기반의 레디스는 해당 스크립트를 실행하느라 다른 요청을 처리하지 못하기 때문)
    ``` lua
    local counter = tonumber(redis.call(‘get’, KEYS[1]))
    if counter < tonumber(ARGV[1]) then
     redis.call(‘incr’, KEYS[1])
     redis.call(‘expire’, KEYS[1], ARGV[2])
     return 1
    else
     return 0
    end
    ```
- 정렬 집합 자료구조 사용
  - 요청의 타임스탬프를 sorted set에 추가함으로써 특정 시간 window 내에 들어오는 요청의 속도를 정확하게 제어 가능
  - `ZREMRANGEBYSCORE`: `ZREMRANGEBYSCORE` 명령어를 제공하여 오래된 타임스탬프를 효율적으로 제거하여 성능을 효율적으로 유지

#### 동기화 이슈
수백만 사용자의 요청을 처리하려면 한대의 처리율 제한 장치 서버로는 충분하지 않을 수 있음 -> 여러 대의 처리율 제한 장치 서버를 둠 -> 동기화 필수

웹 계층은 stateless이므로 오른쪽 그림처럼 각기 다른 제한 장치로 보낼 수 있음, 이때 동기화를 하지 않는다면 제한 장치 1은 클라이언트 2에 대해서는 아무것도 모르므로 처리율 제한을 올바르게 수행할 수 없을 것

![스크린샷 2024-05-19 오후 5 54 02](https://github.com/warnus/devocean-system-study-2024/assets/58351498/0f943986-610e-4e4b-b7f5-aef436687f9f)

해결책: sticky session을 활용하여 같은 클라이언트로부터의 요청은 항상 같은 처리율 제한 장치로 보낼 수 있도록 하는 것
- 규모면에서 확장 가능하지도 않으며 유연하지도 않은 설계

더 나은 해결책: 레디스와 같은 중앙 집중형 데이터 저장소 활용

![스크린샷 2024-05-19 오후 5 58 00](https://github.com/warnus/devocean-system-study-2024/assets/58351498/facd4678-3f95-4f6b-9db8-2a0fbb8e4fe2)

#### 성능 최적화
**첫번째 개선점**

데이터센터를 지원한느 문제는 처리율 제한 장치에 매우 중요한 문제, 데이터센터에서 멀리 떨어진 사용자를 지원하려다 보면 지연시간이 증가할 수 밖에 없기 때문

> 클라우드플레어는 지역적으로 분산된 194곳의 위치에 엣지 서버를 설치해 두어 사용자의 트래픽을 가장 가까운 엣지 서버로 전달해 지연시간을 줄임

**두번쨰 개선점**
제한 장치 간에 데이터를 동기화할 때 최종 일관성 모델을 사용하는 것

#### 모니터링
처리율 제한 장치가 잘 동작하고 있는지 확인하기 위해 데이터를 수집하고 모니터링
- 채택된 처리율 제한 알고리즘이 효과적
- 정의한 처리율 제한 규칙이 효과적

처리율 제한 규칙이 너무 빡빡하게 설정되었다면 많은 유효 요청이 처리되지 못하고 버려질 것 -> 규칙을 완화할 필요가 있음

> 깜짝 세일 이벤트일 때, 트래픽이 급증하여 처리율 제한 장치가 비효율적으로 동작한다면 그런 트래픽 패턴을 잘 처리할 수 있도록 알고리즘을 바꾸는 것을 생각해봐야 함
>
> 이 경우에는 토큰 버킷 알고리즘이 적합할 것

## 4단계) 마무리
- 경성(hard), 연성(soft) 처리율 제한
  - 경성 처리율 제한: 요청의 개수는 임계치를 절대 넘어설 수 없음
  - 연성 처리율 제한: 요청 개수는 잠시 동안은 임계치를 넘어설 수 없음
- 다양한 계층에서의 처리율 제한
  - 애플리케이션 계층말고도 다른 계층에서도 처리율 제한 가능
  - Iptables를 사용하면 IP주소로 처리율 제한을 적용하는 것이 가능
- 처리율 제한을 회피하는 방법, 클라이언트를 어떻게 설계하는 것이 최선인가?
  - 클라이언트 측 캐시를 사용해 API 호출 횟수를 줄임
  - 처리율 제한의 임계치를 이해하고 짧은 시간 동안 너무 많은 메시지를 보내지 않도록 함
  - 예외나 에러를 처리하는 코드를 도입하여 클라이언트가 예외적 상황으로 gracefully 복구될 수 있도록 함
  - 재시도 로직을 구현할 때는 충분한 백오프 시간을 둠

# 참고
[Redis Lua Script를 이용해서 API Rate Limiter개발](https://dev.gmarket.com/69)
[Implementing Scalable Rate Limiting in a Distributed Environment with Lua Scripts and Redis Sorted Sets](https://mahdie-asiyaban.medium.com/implementing-scalable-rate-limiting-in-a-distributed-environment-with-lua-scripts-and-redis-sorted-3d743ab8734a)
