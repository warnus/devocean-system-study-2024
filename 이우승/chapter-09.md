
# 웹 크롤러 설계

- Robot또는 Spider라고도 부름
- 검색엔진에서 널리 쓰이며 웹의 새로 갱신된 콘텐츠를 찾아내는 것이 주 목적
- 콘텐츠는 웹페이지, 이미지, 비디오 또는 PDF 등

크롤러 사용 유형
- 검색 엔진 인덱싱(Search Engine Indexing)
	- 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스 생성
	- Googlebot은 구글 검색 엔진이 사용하는 웹 크롤러
- 웹 아카이빙(Web Archiving)
	- 웹페이지를 장기보관 하기 위해 정보를 모으는 절차
	- 많은 국립 도서관이 크롤러를 돌려 웹 사이틀 아카이빙 중
- 웹 마이닝(Web Mining)
	- 인터넷에서 유용한 지식을 도출해내는 작업
	- 유명 금융 기업들은 크롤러를 사용해 주주총회 자료나 연차 보고서를 다운받아 핵심 사업 방향 도출
- 웹 모니터링(Web Monitoring)
	- 저작권이나 상표권 침해 사례 모니터링
	- 디지마크(Digimarc)사는 웹 크롤러로 해적판 저작물을 찾아내서 보고

## 문제 이해 및 설계 범위 확정

### 웹 크롤러 기본 알고리즘

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지 다운로드
2. 다운 받은 웹 페이지에서 URL 추출
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위 과정을 처음부터 반복

### 크롤러 요구사항

- 규모 확장성
	- 수십억개의 페이지를 크롤링하기 위해 병행성을 활용하면 효과적으로 크롤링 가능
- 안정성
	- 반응 없는 서버, 장애, 악성 코드들을 잘 대응할 수 있어야함
- 예절(politeness)
	- 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 하면 안 됨
- 확장성
	- 새로운 형태의 콘텐츠를 지원하기 쉬워야함


### 개략적 규모 추정

- 매달 10억 개의 웹페이지 다운로드
- QPS=10억/30일/24시간/3600초 = 대략 400페이지/초
- 최대(Peak) QPS = 2 X QPS = 800
- 웹페이지 크기 평균은 500k라고 가정
- 10억 페이지 x 500k = 500TB/월
- 5년간 보관한다고 가정하면 500TB x 12개월 x 5년 = 30PB

## 개략적 설계안 제시 및 동의 구하기

![](/이우승/assets/ch-09/ch09_01.jpeg)

### 시작 URL 집합

- 웹 크롤러가 크롤링을 시작하는 출발점

### 미수집 URL 저장소

- 다운로드할 URL을 저장 관리하는 컴포넌트
- FIFO Queue 구조

### HTML 다운로더

- 웹페이지를 다운로드하는 컴포넌트
- 다운로드할 페이지는 미수집 URL 저장소가 제공

### 도메인 이름 변환기

- URL을 IP 주소로 변환


### 콘텐츠 파서

- 파싱과 검증 절차 진행
- 이상한 웹페이지를 걸러냄

### 중복 콘텐츠인가?

- 웹상의 29% 가량의 콘텐츠는 중복된 자료들
- 중복을 가려내기 위해 웹피이지의 해시 값을 비교하는 방법 사용 가능

### 콘텐츠 저장소

- HTML 문서를 보관하는 시스템
- 본 설계안에서는 디스크와 메모리를 동시 사용
	- 데이터 양이 너무 많아 대부분의 콘텐츠는 디스크 저장
	- 인기 있는 콘텐츠는 메모리 저장

### URL 추출기

- HTML 페이지를 파싱하여 링크들을 골라냄
- 상대 경로는 전부 절대 경로로 변환

### URL 필터

- 다음과 같은 URL들을 크롤링 대상에서 제외
	- 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL
	- 접속시 오류가 발생하는 URL
	- 접근 제외 목록에 포함된 URL

### 이미 방문한 URL?

- 이미 방문한 적 있는 URL을 추적하여 여러번 처리하거나 무한루프에 빠지는 것을 방지
- 블룸 필터나 해시 테이블을 자료구조로 사용 가능


### URL 저장소

- 이미 방문한 URL을 보관하는 저장소


## 상세 설계

### DFS를 쓸 것인가, BFS를 쓸 것인가

- 웹은 유향(directed graph) 그래프
	- 페이지는 노드, 하이퍼링크는 에지(edge)
- 크롤링은 유향 그래프를 에지를 따라 탐색하는 과정
- DFS, BFS는 그래프 탐색에 널리 사용되는 알고리즘
- DFS(depth-first search)는 그래프 크기가 클 경우 어느 정도로 깊이 가게 될지 가늠하기 어려움
- 웹 크롤러는 보통 BFS(breadth-first search)를 사용

BFS를 사용하는 경우 다음과 같은 문제 발생

- 링크들을 병렬로 처리하는 경우 같은 서버에 수많은 요청을 하여 과부하게 걸리게됨
- BFS는 URL간 우선순위가 없음

### 미수집 URL 저장소

- 미수집 URL 저장소를 활용하여 위 문제 개선 가능
- URL 사이의 우선순위와 freshness를 구별하는 크롤러 구성 가능

예의

- 웹 크롤러는 수집 대상 서버에 짧은 시간 안에 너무 많은 요청을 보내는 것을 감가해야함
- DoS 공격으로 간주될 수 있음
- 한 번에 한 페이지만 요청하는 구조가 되어야함
- 웹 사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지해 구현

![](/이우승/assets/ch-09/ch09_02.jpeg)

- 큐 라우터(queue router)
	- 같은 호스트에 속한 URL은 언제나 같은 큐(b1, b2, ...bn)로 가도록 보장
- 매핑 테이블(mapping table)
	- 호스트 이름과 큐 사이의 관계를 보관하는 테이블
- FIFO Queue
	- 같은 호스트에 속한 URL은 언제나 같은 큐에 보관
- 큐 선택기(queue selector)
	- 큐들을 순회하면서 큐에서 URL을 꺼내 작업 스레드에 전달
- 작업 스레드(worker thread)
	- 전달된 URL을 다운로드하는 작업 수행

###  우선순위

- 유용성에 따라 URL의 우선순위를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도 사용

![](/이우승/assets/ch-09/ch09_03.jpeg)

- 전면 큐 : 우선순위 결정 과정 처리
- 후면 큐 : 크롤러가 예의 바르게 동작하도록 보증

### 신선도(freshness)

- 웹페이지는 수시로 추가, 삭제, 변경 됨
- freshness를 유지하기 위해 이미 다운로드한 페이지라도 주기적으로 재수집(recrawl) 해야함
- 모든 URL을 재수집하는 것은 많은 리소스 필요
- 최적화 방안
	- 웹 페이지의 변경 이력 활용
	- 우선순위를 활용하여 중요한 페이지는 좀 더 자주 재수집

### HTML 다운로더

#### Robots.txt

- 이 파일에는 크롤러가 수집해도 되는 페이지 목록들 존재

#### 성능 최적화

- 분산 크롤링
	- 각 서버가 여러 스레드를 돌려 다운로드 작업 처리
- 도메인 이름 변환 결과 캐시
	- DNS 요청 처리에 보통 10ms ~ 200ms 소요
	- 도메인 이름과 IP 주소 관계를 캐시 보관
	- 크론잡 등으로 주기적으로 갱신
- 지역성
	- 크롤링 서버가 크롤링 대상 서버와 지역적으로 가까우면 다운로드 시간이 개선됨
	- 지역성 활용 전략은 크롤 서버, 캐시, 큐, 저장소 등 대부분의 컴포넌트에 적용 가능
- 짧은 타임아웃
	- 응답이 느리거나 응답이 없는 서버를 걸러냄
- 안정성
	- 안정해시 : 부하 분산에 사용
	- 크롤링 상태 및 수집 데이터 저장 : 장애가 발생해도 쉽게 복구할 수 있도록 기록
	- 예외처리 : 전체 시스템이 중단되지 않도록 고려
	- 데이터 검증 : 시스템 오류 방지를 위해 검증 작업 수행

### 문제 있는 콘텐츠 감지 및 회피

- 중복 콘텐츠
	- 해시나 체크섬 등을 사용하여 중복 콘텐츠 탐지
- 거미 덫(spider trap)
	- 크롤러를 무한 루프에 빠지도록 설계된 웸 피이지
	- URL 최대 길이를 제한하여 회피 가능
- 데이터 노이즈
	- 광고나 스크립트 코드, 스펨 URL 같은 것은 불필요하여 제외 해야함


# 참고

크롤링
- https://velog.io/@mowinckel/%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%A7%81-I